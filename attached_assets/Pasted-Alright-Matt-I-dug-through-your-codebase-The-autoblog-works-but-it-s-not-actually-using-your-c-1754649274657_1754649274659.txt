Alright Matt — I dug through your codebase. The autoblog “works,” but it’s not actually using your curated OpenAI Assistant, so your carefully crafted prompt rules never make it into the model. That’s why you’re getting generic fluff.

What’s going wrong (clear + fixable)
Root cause	Evidence in repo	Why it causes generic output	The fix
Your Assistant isn’t being used	.env and .env.local contain no OPENAI_ASSISTANT_ID or OPENAI_API_KEY. In server/routes.ts you rely on process.env.OPENAI_ASSISTANT_ID and process.env.OPENAI_API_KEY.	When assistantId is missing, generateContentWithAssistant() falls back to chat completions with a thin system prompt. That ignores your Assistant’s instructions/context.	Add valid secrets: OPENAI_API_KEY and OPENAI_ASSISTANT_ID. Verify at runtime.
No hard enforcement of JSON/structure in fallback	Fallback uses openai.chat.completions.create({ model: "gpt-4o", messages: [...] }) with no response_format.	Model “tries” to follow your JSON spec, often returns waffle.	Use Responses API with response_format: { type: "json_object" }, or (minimal change) set response_format in chat if supported in your SDK version.
Your prompt rules aren’t injected into the Assistant run	In runs.create(...) you pass only { assistant_id }.	The Assistant will use its own base instructions, but your job-specific blog template and SEO rules are only in the message, so adherence is weaker.	Pass your strict rules via instructions: assistantPrompt on the run (this is additive to base instructions).
Might be grabbing the wrong message from the thread	You do messages.list(thread.id) then find(msg => msg.role === "assistant").	If ordering flips, you might pick an older assistant reply.	Sort by created_at desc and pick the newest assistant message.
Images only “described,” not seen by the Assistant	You first analyze images via a separate vision call, then append the analysis as text.	If the vision step fails or is weak, generation becomes generic.	Also send images directly in the Assistant thread (alongside the text). Data URLs are fine.

Quick env checklist (do this first)
Add these secrets where your server actually runs (Replit Secrets / Vercel / .env on your server):

bash
Copy
OPENAI_API_KEY=sk-***
OPENAI_ASSISTANT_ID=asst_***
DATABASE_URL=postgres://...
PRIVATE_OBJECT_DIR= # already required by object storage
PUBLIC_OBJECT_DIR=  # if you need public bucket fallback
Bonus: add a tiny health-check route to echo whether assistantId is present and which model is used. Speeds up debugging.

Drop-in code fixes (copy/paste in VS Code)
File: server/routes.ts

Force Assistant usage with your prompt + pick the latest message + keep a robust fallback

Replace your existing generateContentWithAssistant function with this version:

ts
Copy
// === START PATCH: Stronger Assistant-first with strict JSON + safe fallback ===
async function generateContentWithAssistant(prompt: string, imageUrls: string[] = []) {
  const assistantId = process.env.OPENAI_ASSISTANT_ID;
  const haveAssistant = Boolean(assistantId && assistantId.trim().length > 0);

  // Build one consolidated user message with optional inline images
  const contentParts: any[] = [{ type: "text", text: prompt }];

  for (const url of imageUrls ?? []) {
    // data URLs work here
    contentParts.push({ type: "image_url", image_url: { url } });
  }

  // Prefer Assistant API if configured
  if (haveAssistant) {
    try {
      const thread = await openai.beta.threads.create();

      await openai.beta.threads.messages.create(thread.id, {
        role: "user",
        content: contentParts,
      });

      // **Key change**: inject your stricter blog spec as run-time instructions too
      const run = await openai.beta.threads.runs.create(thread.id, {
        assistant_id: assistantId!,
        instructions: `${prompt}\n\nReturn a SINGLE JSON object only.`,
        // metadata: { feature: "autoblog" } // optional debugging
      });

      // Poll until it completes (unchanged logic, but more defensive)
      let attempts = 0;
      const maxAttempts = 60;
      let runStatus = await openai.beta.threads.runs.retrieve(thread.id, run.id);

      while ((runStatus.status === "queued" || runStatus.status === "in_progress" || runStatus.status === "running") && attempts < maxAttempts) {
        await new Promise(r => setTimeout(r, 1000));
        runStatus = await openai.beta.threads.runs.retrieve(thread.id, run.id);
        attempts++;
        if (attempts % 10 === 0) console.log(`Run status: ${runStatus.status} (attempt ${attempts})`);
      }

      if (runStatus.status === "completed") {
        const messages = await openai.beta.threads.messages.list(thread.id, { order: "desc", limit: 10 });
        const assistantMessage = messages.data.find(m => m.role === "assistant");

        if (assistantMessage?.content?.[0]?.type === "text") {
          return assistantMessage.content[0].text.value;
        }

        console.warn("Assistant returned no text block — falling back to chat.");
      } else {
        console.warn(`Assistant run ended with status ${runStatus.status} — falling back to chat.`);
      }
    } catch (err: any) {
      console.warn("Assistant API failed — falling back to chat completions:", err?.message || err);
    }
  } else {
    console.warn("OPENAI_ASSISTANT_ID not set — using chat completion fallback.");
  }

  // Fallback: force JSON output using Responses API if available, else tight chat prompt
  try {
    // Prefer Responses API for structured output (SDK v4+)
    // @ts-ignore if your SDK is older
    const resp = await (openai as any).responses.create({
      model: "gpt-4o-mini",
      input: [{ role: "user", content: contentParts }],
      response_format: { type: "json_object" },
      temperature: 0.2,
    });

    // Responses API text output is under output[0].content[0].text
    const out = resp.output?.[0]?.content?.[0]?.text ?? resp.output_text;
    if (out) return out;
  } catch (e) {
    console.warn("Responses API not available, using chat.completions:", (e as any)?.message);
  }

  // Last resort: chat.completions with a strict system and low temperature
  const response = await openai.chat.completions.create({
    model: "gpt-4o",
    messages: [
      {
        role: "system",
        content: "You generate ONLY a single valid JSON object with fields: title, excerpt, content (HTML), slug. No prose before/after.",
      },
      { role: "user", content: prompt }
    ],
    temperature: 0.2
    // If your SDK supports it: response_format: { type: "json_object" }
  });

  return response.choices?.[0]?.message?.content ?? "{}";
}
// === END PATCH ===
Use the Assistant for image context as well (optional but recommended)
You’re currently doing a separate vision call and then pasting those notes into the prompt. That’s fine, but the consolidated contentParts (text + images) above is stronger and reduces “generic” drift.

Improve the blog generate handler’s parsing robustness
You already try-catch and JSON extract. Keep that, but after my patch the model is far more likely to return clean JSON.

Sanity checks to run (2 minutes, do them now)
Log which path was used: add console.log("Using Assistant:", haveAssistant) at the start of generateContentWithAssistant.

Generate a post from Admin → Autoblog and watch server logs:

If you see “OPENAI_ASSISTANT_ID not set,” fix secrets.

If Assistant runs, you should see a successful status and a JSON blob.

Confirm that the created blogPosts.content is HTML from JSON, not a raw paragraph.

Why it felt generic (in plain English)
You weren’t actually calling your tuned Assistant most of the time.

The fallback wasn’t forced into a schema, so it rambled.

Image context wasn’t guaranteed to be consumed by the model.

No strong “latest message only” guard when reading the thread.

Tighten those four and your blogs start sounding like you, not a sleepy robot.